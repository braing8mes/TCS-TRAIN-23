{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Description\nHello, I am ihsan.<br>\nThis notebook is about Knn(K-Nearest Neighbors). I am learning Machine Learning algorithms and This notebook is my third notebook about Machine Learning. I wrote about Regression two notebooks. I will share them end of the description. In this notebook, I made the data review and data visualization and I applied the KNN(K-Nearest Neighbors) model of the classification part from machine learning in this notebook. If you review this notebook and interpret it for my development, I will be so happy. I will be waiting for your comments.<br>\n\n[For a detailed description of the dataset, please click here](https://www.kaggle.com/datasets/uciml/glass)\n\nOther my notebook:\n- [GPU Search](https://www.kaggle.com/code/ihsncnkz/gpu-search)\n- [EDA And Linear Regression](https://www.kaggle.com/code/ihsncnkz/eda-and-linear-regression)\n- [Regression Models](https://www.kaggle.com/code/ihsncnkz/regression-models)\n- [Logistic Regression](https://www.kaggle.com/code/ihsncnkz/logistic-regression)\n","metadata":{}},{"cell_type":"markdown","source":"# Contents\n- [Data Review](#1)\n- [Data Visualization](#2)\n- [K-Nearest Neighbors(KNN)](#3)\n    - [Data preparing for KNN](#knn1)\n    - [Training Model](#knn2)","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-06T03:18:17.779991Z","iopub.execute_input":"2022-08-06T03:18:17.780806Z","iopub.status.idle":"2022-08-06T03:18:19.579822Z","shell.execute_reply.started":"2022-08-06T03:18:17.780664Z","shell.execute_reply":"2022-08-06T03:18:19.576326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Review <a id = \"1\"></a>","metadata":{}},{"cell_type":"markdown","source":"Firstly, I will review dataset.","metadata":{}},{"cell_type":"code","source":"# Reading data with pandas library.\ndata = pd.read_csv(\"/kaggle/input/glass/glass.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-08-06T03:18:19.583999Z","iopub.execute_input":"2022-08-06T03:18:19.585215Z","iopub.status.idle":"2022-08-06T03:18:19.611414Z","shell.execute_reply.started":"2022-08-06T03:18:19.585126Z","shell.execute_reply":"2022-08-06T03:18:19.610282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2022-08-06T03:18:19.6129Z","iopub.execute_input":"2022-08-06T03:18:19.613507Z","iopub.status.idle":"2022-08-06T03:18:19.668979Z","shell.execute_reply.started":"2022-08-06T03:18:19.613467Z","shell.execute_reply":"2022-08-06T03:18:19.666843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I am looking to data top ten.\ndata.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-08-06T03:18:19.673678Z","iopub.execute_input":"2022-08-06T03:18:19.675391Z","iopub.status.idle":"2022-08-06T03:18:19.718083Z","shell.execute_reply.started":"2022-08-06T03:18:19.675293Z","shell.execute_reply":"2022-08-06T03:18:19.715835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# İnformation of Data\ndata.info()","metadata":{"execution":{"iopub.status.busy":"2022-08-06T03:18:19.720253Z","iopub.execute_input":"2022-08-06T03:18:19.722017Z","iopub.status.idle":"2022-08-06T03:18:19.774432Z","shell.execute_reply.started":"2022-08-06T03:18:19.721955Z","shell.execute_reply":"2022-08-06T03:18:19.769682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dataset Values(Max, min, std..)\ndata.describe()","metadata":{"execution":{"iopub.status.busy":"2022-08-06T03:18:19.77764Z","iopub.execute_input":"2022-08-06T03:18:19.779096Z","iopub.status.idle":"2022-08-06T03:18:19.852551Z","shell.execute_reply.started":"2022-08-06T03:18:19.77903Z","shell.execute_reply":"2022-08-06T03:18:19.850756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data Correlation: Relationship between columns\ndata.corr()","metadata":{"execution":{"iopub.status.busy":"2022-08-06T03:18:19.855959Z","iopub.execute_input":"2022-08-06T03:18:19.857639Z","iopub.status.idle":"2022-08-06T03:18:19.893119Z","shell.execute_reply.started":"2022-08-06T03:18:19.857561Z","shell.execute_reply":"2022-08-06T03:18:19.891227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualization of correlation result with seaborn library heatmap.\nf, ax = plt.subplots(figsize = (12,10))\nsns.heatmap(data.corr(), annot = True, linewidths = 0.5, linecolor = \"black\", fmt = \".4f\", ax = ax)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-06T03:18:19.896184Z","iopub.execute_input":"2022-08-06T03:18:19.896803Z","iopub.status.idle":"2022-08-06T03:18:20.954267Z","shell.execute_reply.started":"2022-08-06T03:18:19.896725Z","shell.execute_reply":"2022-08-06T03:18:20.953154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When I looked at correlation I  generally am seeing inverse proportion in between the columns.","metadata":{}},{"cell_type":"code","source":"# Visualization of correlation results with seaborn library pairplot.\nsns.pairplot(data, hue = 'Type')","metadata":{"execution":{"iopub.status.busy":"2022-08-06T03:18:20.955918Z","iopub.execute_input":"2022-08-06T03:18:20.956369Z","iopub.status.idle":"2022-08-06T03:18:55.886011Z","shell.execute_reply.started":"2022-08-06T03:18:20.956329Z","shell.execute_reply":"2022-08-06T03:18:55.884373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the code above, When I looked, I am seeing have seven types in the dataset. These types are types of glass. It has explained the type of glass in the explanation of the dataset.If want to get information on types of glass In Description, By clicking (For a detailed description of the dataset, please click here) you get information. At the same time, I am seeing the correlation of columns and the distribution of data.","metadata":{}},{"cell_type":"code","source":"# Dataset columns names\ndata.columns","metadata":{"execution":{"iopub.status.busy":"2022-08-06T03:18:55.890763Z","iopub.execute_input":"2022-08-06T03:18:55.892365Z","iopub.status.idle":"2022-08-06T03:18:55.905795Z","shell.execute_reply.started":"2022-08-06T03:18:55.892254Z","shell.execute_reply":"2022-08-06T03:18:55.903859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Visualization <a id = \"2\"></a>","metadata":{}},{"cell_type":"markdown","source":"In this chapter, I made a visualization of data.","metadata":{}},{"cell_type":"code","source":"# I visualized the number of type in the dataset!\ndataType = data[\"Type\"].value_counts(dropna=False)\ndataTypedf = np.array(dataType)\nx = list(dataTypedf)\ny = data.Type.value_counts().index\n\nplt.figure(figsize=(7,5))\nsns.barplot(x = y, y = x, palette = sns.cubehelix_palette(len(x)))\nplt.xlabel(\"Number of Type\")\nplt.ylabel(\"Number\")\nplt.title(\"The Number Of Type In Data\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-06T03:18:55.9087Z","iopub.execute_input":"2022-08-06T03:18:55.909938Z","iopub.status.idle":"2022-08-06T03:18:56.18899Z","shell.execute_reply.started":"2022-08-06T03:18:55.909865Z","shell.execute_reply":"2022-08-06T03:18:56.187113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"One = data[data.Type == 1]\nTwo = data[data.Type == 2]\nThree = data[data.Type == 3]\nFour = data[data.Type == 4]\nFive = data[data.Type == 5]\nSix = data[data.Type == 6]\nSeven = data[data.Type == 7]\n\nplt.figure(figsize=(10,10))\nplt.scatter(One.Si, One.Al, color = \"red\", label = \"One\")\nplt.scatter(Two.Si, Two.Al, color = \"blue\", label = \"Two\")\nplt.scatter(Three.Si, Three.Al, color = \"orange\", label = \"Three\")\nplt.scatter(Four.Si, Four.Al, color = \"black\", label = \"Four\")\nplt.scatter(Five.Si, Five.Al, color = \"yellow\", label = \"Five\")\nplt.scatter(Six.Si, Six.Al, color = \"purple\", label = \"Six\")\nplt.scatter(Seven.Si, Seven.Al, color = \"green\", label = \"Seven\")\nplt.xlabel(\"Sİ\")\nplt.ylabel(\"AL\")\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-06T03:18:56.192431Z","iopub.execute_input":"2022-08-06T03:18:56.19301Z","iopub.status.idle":"2022-08-06T03:18:56.647318Z","shell.execute_reply.started":"2022-08-06T03:18:56.192964Z","shell.execute_reply":"2022-08-06T03:18:56.645765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this visualization, I want to show the distribution of data.","metadata":{}},{"cell_type":"markdown","source":"# K-Nearest Neighbors(KNN) <a id = \"3\" ></a>\n\nIn this chapter, I will apply KNN(K-Nearest Neighbors) model.<br>\n\n**What is The KNN(K-Nearest Neighbors)?**<br>\n\nThe k-nearest neighbors algorithm, also known as KNN or k-NN, is a non-parametric, supervised learning classifier, which uses proximity to make classifications or predictions about the grouping of an individual data point. <br>\n","metadata":{}},{"cell_type":"markdown","source":"## Data preparing for KNN <a id = \"knn1\"></a>","metadata":{}},{"cell_type":"markdown","source":"I prepare x_data and y_data and I will normalize to x_data. The reason I do normalization is that the values do not match with each other, so the number 72.61 and the number 1.59 are not in the same range, I will reduce these numbers to the same range. My range will be 0-1.","metadata":{}},{"cell_type":"code","source":"# x_data\nx_data = data.drop([\"Type\"],axis = 1)\n\n# y_data\ny_data = data.Type.values","metadata":{"execution":{"iopub.status.busy":"2022-08-06T03:18:56.649154Z","iopub.execute_input":"2022-08-06T03:18:56.649766Z","iopub.status.idle":"2022-08-06T03:18:56.659643Z","shell.execute_reply.started":"2022-08-06T03:18:56.649691Z","shell.execute_reply":"2022-08-06T03:18:56.65773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_data","metadata":{"execution":{"iopub.status.busy":"2022-08-06T03:18:56.662389Z","iopub.execute_input":"2022-08-06T03:18:56.663305Z","iopub.status.idle":"2022-08-06T03:18:56.697816Z","shell.execute_reply.started":"2022-08-06T03:18:56.66325Z","shell.execute_reply":"2022-08-06T03:18:56.696242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_data","metadata":{"execution":{"iopub.status.busy":"2022-08-06T03:18:56.699902Z","iopub.execute_input":"2022-08-06T03:18:56.700852Z","iopub.status.idle":"2022-08-06T03:18:56.712699Z","shell.execute_reply.started":"2022-08-06T03:18:56.700806Z","shell.execute_reply":"2022-08-06T03:18:56.710835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Normalization\nx_data = (x_data - np.min(x_data))/(np.max(x_data) - np.min(x_data)).values\nx_data","metadata":{"execution":{"iopub.status.busy":"2022-08-06T03:18:56.715374Z","iopub.execute_input":"2022-08-06T03:18:56.716587Z","iopub.status.idle":"2022-08-06T03:18:56.748481Z","shell.execute_reply.started":"2022-08-06T03:18:56.716522Z","shell.execute_reply":"2022-08-06T03:18:56.74729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train test split\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.3, random_state=1)","metadata":{"execution":{"iopub.status.busy":"2022-08-06T03:18:56.74988Z","iopub.execute_input":"2022-08-06T03:18:56.751272Z","iopub.status.idle":"2022-08-06T03:18:56.917358Z","shell.execute_reply.started":"2022-08-06T03:18:56.751221Z","shell.execute_reply":"2022-08-06T03:18:56.916036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training Model <a id = \"knn2\"></a>","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=7)\n\nknn.fit(x_train, y_train)\n\npretiction = knn.predict(x_test)\n\nprint(\"{} knn score: {}\".format(7,knn.score(x_test, y_test)))","metadata":{"execution":{"iopub.status.busy":"2022-08-06T03:18:56.918927Z","iopub.execute_input":"2022-08-06T03:18:56.919659Z","iopub.status.idle":"2022-08-06T03:18:57.092919Z","shell.execute_reply.started":"2022-08-06T03:18:56.919618Z","shell.execute_reply":"2022-08-06T03:18:57.091337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, choosing the variable n_neighbors as seven because there are seven types of glass in the data set.","metadata":{}},{"cell_type":"code","source":"test_score_list = []\ntrain_score_list = []\n\nfor i in range(1,15):\n    knn2 = KNeighborsClassifier(n_neighbors = i)\n    knn2.fit(x_train, y_train)\n    test_score_list.append(knn2.score(x_test, y_test))\n    train_score_list.append(knn2.score(x_train, y_train))\n    \nplt.figure(figsize=(12,5))\np = sns.lineplot(range(1,15),train_score_list,marker='*',label='Train Score')\np = sns.lineplot(range(1,15),test_score_list,marker='o',label='Test Score')","metadata":{"execution":{"iopub.status.busy":"2022-08-06T03:18:57.095309Z","iopub.execute_input":"2022-08-06T03:18:57.095854Z","iopub.status.idle":"2022-08-06T03:18:57.652836Z","shell.execute_reply.started":"2022-08-06T03:18:57.095802Z","shell.execute_reply":"2022-08-06T03:18:57.650371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I want to see the best parameter of n_neighbors. So I use the GridSearchCV model to see the best parameter of n_neighbors.","metadata":{}},{"cell_type":"code","source":"#import GridSearchCV\nfrom sklearn.model_selection import GridSearchCV\n\n#In case of classifier like knn the parameter to be tuned is n_neighbors\nparam_grid = {'n_neighbors':np.arange(1,50)}\n\nknn3 = KNeighborsClassifier()\nknn_cv= GridSearchCV(knn3,param_grid,cv=5)\nknn_cv.fit(x_test,y_test)\n\nprint(\"Best Score:\" + str(knn_cv.best_score_))\nprint(\"Best Parameters: \" + str(knn_cv.best_params_))","metadata":{"execution":{"iopub.status.busy":"2022-08-06T03:18:57.655597Z","iopub.execute_input":"2022-08-06T03:18:57.656242Z","iopub.status.idle":"2022-08-06T03:18:59.369694Z","shell.execute_reply.started":"2022-08-06T03:18:57.656188Z","shell.execute_reply":"2022-08-06T03:18:59.368015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CONCLUSION\nI am learning the English language newly if I make an errata please advise me in the comment. Thank you for reading my notebook, your votes and your comments. I will be waiting for your advice.","metadata":{}}]}